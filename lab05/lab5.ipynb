{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Sequence\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def append_ones(matrix):\n",
    "#     return np.concatenate((matrix, np.ones((matrix.shape[0], 1), dtype=np.float32)), axis=1)\n",
    "\n",
    "\n",
    "def zeros(*dims: int) -> NDArray:\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "\n",
    "def ones(*dims: int) -> NDArray:\n",
    "    return np.ones(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "\n",
    "def rand(*dims: int) -> NDArray:\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "\n",
    "def chunks(seq: NDArray, size: int):\n",
    "    return (seq[pos : pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "\n",
    "def tiles(examples: NDArray):\n",
    "    space = 2\n",
    "    rows, cols, h, w = examples.shape\n",
    "\n",
    "    img_matrix = np.empty(shape=(rows * (h + space) - space, cols * (h + space) - space))\n",
    "    img_matrix.fill(np.nan)\n",
    "\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            x_0 = r * (h + space)\n",
    "            y_0 = c * (w + space)\n",
    "            ex_min = np.min(examples[r, c])\n",
    "            ex_max = np.max(examples[r, c])\n",
    "            img_matrix[x_0 : x_0 + h, y_0 : y_0 + w] = (examples[r, c] - ex_min) / (ex_max - ex_min)\n",
    "\n",
    "    plt.matshow(img_matrix, cmap=\"gray\", interpolation=\"none\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnists import MNIST\n",
    "\n",
    "mnist = MNIST()\n",
    "digits = mnist.train_images()\n",
    "digits = digits[: 12 * 24]\n",
    "digits = digits.reshape(-1, 24, 28, 28)\n",
    "tiles(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machine & Contrastive Divergence algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x: NDArray) -> NDArray:\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, vsize: int, hsize: int, lr: float, momentum: float):\n",
    "        # Variables' sizes\n",
    "        self.vsize = vsize\n",
    "        self.hsize = hsize\n",
    "        # Training hyper-params\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        # Initialize\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Xavier initialization\n",
    "        scale = np.sqrt(6 / (self.vsize + self.hsize))\n",
    "        self.w = np.random.uniform(-scale, +scale, size=(self.vsize, self.hsize)).astype(np.float32)\n",
    "\n",
    "        # Zero initialization\n",
    "        self.b = zeros(self.vsize)\n",
    "        self.c = zeros(self.hsize)\n",
    "\n",
    "        # Velocity (momentum) tensor initialization\n",
    "        self.m_w = zeros(self.vsize, self.hsize)\n",
    "        self.m_b = zeros(self.vsize)\n",
    "        self.m_c = zeros(self.hsize)\n",
    "\n",
    "    def probas_v(self, h: NDArray) -> NDArray:\n",
    "        return sigmoid(self.b + h @ self.w.T)\n",
    "\n",
    "    def probas_h(self, v: NDArray) -> NDArray:\n",
    "        return sigmoid(self.c + v @ self.w)\n",
    "\n",
    "    def sample(self, v: NDArray, steps: int) -> NDArray:\n",
    "        batch_size = v.shape[0]\n",
    "        # --- Gibbs sampling\n",
    "        for k in range(steps):\n",
    "            h = self.probas_h(v) > rand(batch_size, self.hsize)\n",
    "            σ = self.probas_v(h)\n",
    "            if k < steps - 1:\n",
    "                v = σ > rand(batch_size, self.vsize)\n",
    "\n",
    "        return σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstuction_error(rbm: RBM, v: NDArray) -> float:\n",
    "    batch_size = v.shape[0]\n",
    "    σ = rbm.sample(v, steps=1)\n",
    "    return 1 / batch_size * np.sum((v - σ) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_filters(rbm: RBM):\n",
    "    filters = rbm.w.T\n",
    "    filters = filters.reshape(8, -1, 28, 28)\n",
    "    filters = np.clip(filters, -1, 1)\n",
    "    tiles(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RBM Training with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cdk(rbm: RBM, minibatch: NDArray, k: int = 1):\n",
    "    batch_size = minibatch.shape[0]\n",
    "    v = minibatch\n",
    "\n",
    "    # Compute gradients\n",
    "    # -----------------\n",
    "\n",
    "    # Positive phase\n",
    "    σ = rbm.probas_h(v)\n",
    "\n",
    "    grad_w = -1 / batch_size * (v.T @ σ)\n",
    "    grad_b = -1 / batch_size * (v.sum(axis=0))\n",
    "    grad_c = -1 / batch_size * (σ.sum(axis=0))\n",
    "\n",
    "    # Negative phase\n",
    "\n",
    "    # --- Gibbs sampling\n",
    "    h = σ > rand(batch_size, rbm.hsize)\n",
    "    v = rbm.probas_v(h) > rand(batch_size, rbm.vsize)\n",
    "    for _ in range(k - 1):\n",
    "        h = rbm.probas_h(v) > rand(batch_size, rbm.hsize)\n",
    "        v = rbm.probas_v(h) > rand(batch_size, rbm.vsize)\n",
    "\n",
    "    # --- Negative gradient estimation\n",
    "    σ = rbm.probas_h(v)\n",
    "\n",
    "    grad_w += 1 / batch_size * (v.T @ σ)\n",
    "    grad_b += 1 / batch_size * (v.sum(axis=0))\n",
    "    grad_c += 1 / batch_size * (σ.sum(axis=0))\n",
    "\n",
    "    # Update params\n",
    "    # -------------\n",
    "    rbm.m_w = rbm.momentum * rbm.m_w - rbm.lr * grad_w\n",
    "    rbm.m_b = rbm.momentum * rbm.m_b - rbm.lr * grad_b\n",
    "    rbm.m_c = rbm.momentum * rbm.m_c - rbm.lr * grad_c\n",
    "\n",
    "    rbm.w += rbm.m_w\n",
    "    rbm.b += rbm.m_b\n",
    "    rbm.c += rbm.m_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(rbm: RBM, dataset: NDArray, batch_size: int):\n",
    "    batches_limit = dataset.shape[0] / batch_size\n",
    "    for batch_idx, batch in enumerate(chunks(dataset, batch_size)):\n",
    "        cdk(rbm, batch)\n",
    "        if batch_idx % round(batches_limit / 50) == 0:\n",
    "            print(\"#\", end=\"\")\n",
    "\n",
    "\n",
    "def run_training(rbm: RBM, dataset: NDArray, monitoring_set: NDArray, batch_size: int, epochs_count: int):\n",
    "    for epoch in range(epochs_count):\n",
    "        print(\"Epoch {}:\".format(epoch), end=\"\\t\")\n",
    "\n",
    "        if epoch == 5:\n",
    "            rbm.momentum = 0.8\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_epoch(rbm, dataset, batch_size)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        error = reconstuction_error(rbm, monitoring_set)\n",
    "        print(\"\\telapsed: {0:>2.2f}s, reconstruction error: {1:>2.2f}\".format(elapsed, error))\n",
    "\n",
    "    print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_SIZE = 20_000  # 60000 for whole dataset\n",
    "DIGIT_SIZE = 28\n",
    "\n",
    "VISIBLE_LAYER_SIZE = DIGIT_SIZE * DIGIT_SIZE\n",
    "HIDDEN_LAYER_SIZE = 128\n",
    "\n",
    "mnist_train = mnist.train_images().astype(np.float32) / 255.0\n",
    "np.random.shuffle(mnist_train)\n",
    "dataset = mnist_train[:DATASET_SIZE]\n",
    "dataset = dataset.reshape(DATASET_SIZE, DIGIT_SIZE * DIGIT_SIZE)\n",
    "# dataset = append_ones(dataset)\n",
    "\n",
    "monitoring_indeces = np.random.choice(DATASET_SIZE, 256, replace=False)\n",
    "monitoring_set = dataset[monitoring_indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS_COUNT = 50\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "rbm = RBM(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, LEARNING_RATE, MOMENTUM)\n",
    "draw_filters(rbm)\n",
    "\n",
    "run_training(rbm, dataset, monitoring_set, BATCH_SIZE, EPOCHS_COUNT)\n",
    "\n",
    "draw_filters(rbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DBN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate_up(dbn: Sequence[RBM], n_layers: int, v: NDArray) -> NDArray:\n",
    "    assert 0 <= n_layers < len(dbn)\n",
    "    for i in range(n_layers):\n",
    "        v = dbn[i].probas_h(v)\n",
    "    return v\n",
    "\n",
    "\n",
    "def propagate_dn(dbn: Sequence[RBM], n_layers: int, h: NDArray) -> NDArray:\n",
    "    assert 0 <= n_layers < len(dbn)\n",
    "    for i in reversed(range(n_layers)):\n",
    "        h = dbn[i].probas_v(h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBN reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dbn_reconstuction_error(dbn: Sequence[RBM], n_layers: int, minibatch: NDArray) -> float:\n",
    "    assert 0 <= n_layers < len(dbn)\n",
    "    visible = propagate_up(dbn, n_layers, minibatch)\n",
    "    # batch_size = minibatch.shape[0]\n",
    "    # samples = propagate_dn(dbn, n_layers, samples)\n",
    "    # return 1 / batch_size * np.sum((minibatch - samples) ** 2)\n",
    "    return reconstuction_error(dbn[n_layers], visible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling in Deep Belief Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_rbm(rbm, minibatch, steps):\n",
    "    observations_count = minibatch.shape[0]\n",
    "\n",
    "    # BUG: If you are keeping all the parameters in a single matrix `W`, you should COPY the\n",
    "    # minibatch as the later assignments (that use slicing) MODIFY the input minibatch.\n",
    "    visible = minibatch  # .copy()\n",
    "    hidden = append_ones(zeros(observations_count, rbm.hidden_size))\n",
    "\n",
    "    for cd_i in range(steps):\n",
    "        hidden[:, :-1] = sigmoid(visible @ rbm.W[:, :-1])\n",
    "        hidden[:, :-1] = (hidden[:, :-1] > rand(observations_count, rbm.hidden_size)).astype(np.float32)\n",
    "\n",
    "        visible[:, :-1] = sigmoid(hidden @ np.transpose(rbm.W[:-1, :]))\n",
    "        if cd_i < (steps - 1):\n",
    "            visible[:, :-1] = (visible[:, :-1] > rand(observations_count, rbm.visible_size)).astype(np.float32)\n",
    "\n",
    "    return visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_dbn(dbn: Sequence[RBM], layer_idx: int, minibatch: NDArray, steps: int) -> NDArray:\n",
    "    visible = propagate_up(dbn, layer_idx, minibatch)\n",
    "    samples = dbn[layer_idx].sample(visible, steps)\n",
    "    samples = propagate_dn(dbn, layer_idx, samples)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_layer_samples(dbn: Sequence[RBM], layer_idx: int, minibatch: NDArray, steps: int = 200):\n",
    "    samples = sample_dbn(dbn, layer_idx, minibatch, steps)\n",
    "    samples = samples.reshape(-1, 16, 28, 28)\n",
    "    tiles(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy layer-wise training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dbn(dbn: Sequence[RBM], layer_idx: int, dataset: NDArray, batch_size: int):\n",
    "    dataset = propagate_up(dbn, layer_idx, dataset)\n",
    "    batches_limit = dataset.shape[0] / batch_size\n",
    "    for batch_idx, batch in enumerate(chunks(dataset, batch_size)):\n",
    "        cdk(dbn[layer_idx], batch)\n",
    "        if batch_idx % round(batches_limit / 50) == 0:\n",
    "            print(\"#\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS_COUNT = 50\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "MOMENTUM = 0.5\n",
    "\n",
    "DBN_HIDDEN_LAYER_SIZE = 200\n",
    "dbn = [\n",
    "    RBM(VISIBLE_LAYER_SIZE, DBN_HIDDEN_LAYER_SIZE, LEARNING_RATE, MOMENTUM),\n",
    "    RBM(DBN_HIDDEN_LAYER_SIZE, DBN_HIDDEN_LAYER_SIZE, LEARNING_RATE, MOMENTUM),\n",
    "    RBM(DBN_HIDDEN_LAYER_SIZE, DBN_HIDDEN_LAYER_SIZE, LEARNING_RATE, MOMENTUM),\n",
    "]\n",
    "\n",
    "for layer_idx in range(len(dbn)):\n",
    "    print(\"\\nLearning layer {}\".format(layer_idx))\n",
    "\n",
    "    for epoch in range(EPOCHS_COUNT):\n",
    "        print(\"Epoch {}:\".format(epoch), end=\"\\t\")\n",
    "\n",
    "        if epoch == 5:\n",
    "            dbn[layer_idx].momentum = 0.8\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_dbn(dbn, layer_idx, dataset, BATCH_SIZE)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        error = dbn_reconstuction_error(dbn, layer_idx, monitoring_set)\n",
    "        print(\"\\telapsed: {0:>2.2f}s, reconstruction error: {1:>2.2f}\".format(elapsed, error))\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_layer_samples(dbn, 0, monitoring_set[: 8 * 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_layer_samples(dbn, 1, monitoring_set[: 8 * 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_layer_samples(dbn, 2, monitoring_set[: 8 * 16])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
