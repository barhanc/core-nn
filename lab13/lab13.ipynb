{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from abc import abstractmethod, ABC\n",
    "from typing import Optional, Literal\n",
    "\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zeros(*dims: int) -> NDArray[np.float32]:\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "\n",
    "def ones(*dims: int) -> NDArray[np.float32]:\n",
    "    return np.ones(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "\n",
    "def rand(*dims: int) -> NDArray[np.float32]:\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "\n",
    "def randn(*dims: int) -> NDArray[np.float32]:\n",
    "    return np.random.randn(*dims).astype(np.float32)\n",
    "\n",
    "\n",
    "def sigmoid(x: NDArray) -> NDArray:\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open(\"text8.dat.gz\", \"rb\") as f:\n",
    "    train_dict, train_set, train_tokens = pickle.load(f)\n",
    "\n",
    "train_set = np.random.permutation(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Config = namedtuple(\n",
    "    \"Config\",\n",
    "    (\n",
    "        \"dict_size\",\n",
    "        \"vect_size\",\n",
    "        \"neg_samples\",\n",
    "        \"updates\",\n",
    "        \"learning_rate\",\n",
    "        \"learning_rate_decay\",\n",
    "        \"decay_period\",\n",
    "        \"log_period\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "config = Config(\n",
    "    dict_size=len(train_dict),\n",
    "    vect_size=100,\n",
    "    neg_samples=10,\n",
    "    updates=5_000_000,\n",
    "    learning_rate=0.1,\n",
    "    learning_rate_decay=0.995,\n",
    "    decay_period=10_000,\n",
    "    log_period=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "In order to train distributed word representations, first:\n",
    " - calculate gradient of the cost function with respect to the `word_vector` and store in `word_grad`.\n",
    " - calculate gradient of the cost function with respect to the `context_vect` and store in `context_grad`.\n",
    " - calculate gradient of the cost function with respect to the sampled `negative_vects` and store in neg_context_grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_sample(\n",
    "    config: Config,\n",
    "    train_set: NDArray[np.int64],\n",
    "    train_tokens: NDArray[np.int64],\n",
    ") -> tuple[NDArray, NDArray, float]:\n",
    "    lr = config.learning_rate\n",
    "    loss = 0.0\n",
    "    V_p = randn(config.dict_size, config.vect_size)\n",
    "    V_o = randn(config.dict_size, config.vect_size)\n",
    "\n",
    "    for i in range(config.updates):\n",
    "        w_idx: int = train_set[i % len(train_set), 0]\n",
    "        c_idx: int = train_set[i % len(train_set), 1]\n",
    "        n_idx: NDArray[np.int64] = np.random.randint(0, len(train_tokens), config.neg_samples)\n",
    "        n_idx: NDArray[np.int64] = train_tokens[n_idx]\n",
    "\n",
    "        w = V_p[w_idx, :]  # word vector, shape `(dim,)`\n",
    "        c = V_o[c_idx, :]  # context vector, shape `(dim,)`\n",
    "        n = V_o[n_idx, :]  # sampled noise vectors, shape `(k, dim)`\n",
    "\n",
    "        # Cost and gradient calculation\n",
    "        # -----------------------------\n",
    "        σ_p = sigmoid(+w @ c.T)  # shape `(1,)`\n",
    "        σ_n = sigmoid(-w @ n.T)  # shape `(k,)`\n",
    "        loss -= np.log(σ_p) + np.sum(np.log(σ_n))\n",
    "\n",
    "        if (i + 1) % config.log_period == 0:\n",
    "            print(f\"Update {i+1}\\tLoss: {loss / config.log_period:>2.2f}\")\n",
    "            final_loss = loss / config.log_period\n",
    "            loss = 0.0\n",
    "\n",
    "        grad_w = (σ_p - 1.0) * c + (1.0 - σ_n) @ n\n",
    "        grad_c = (σ_p - 1.0) * w\n",
    "        grad_n = (1.0 - σ_n).reshape(-1, 1) * w\n",
    "\n",
    "        V_p[w_idx, :] -= lr * grad_w\n",
    "        V_o[c_idx, :] -= lr * grad_c\n",
    "        V_o[n_idx, :] -= lr * grad_n\n",
    "\n",
    "        if i % config.decay_period == 0:\n",
    "            lr = lr * config.learning_rate_decay\n",
    "\n",
    "    return V_p, V_o, final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def neg_sample(conf, train_set, train_tokens):\n",
    "#     Vp = randn(conf.dict_size, conf.vect_size)\n",
    "#     Vo = randn(conf.dict_size, conf.vect_size)\n",
    "\n",
    "#     J = 0.0\n",
    "#     learning_rate = conf.learning_rate\n",
    "#     for i in range(conf.updates):\n",
    "#         idx = i % len(train_set)\n",
    "\n",
    "#         word    = train_set[idx, 0]\n",
    "#         context = train_set[idx, 1]\n",
    "        \n",
    "#         neg_context = np.random.randint(0, len(train_tokens), conf.neg_samples)\n",
    "#         neg_context = train_tokens[neg_context]\n",
    "\n",
    "#         word_vect = Vp[word, :]              # word vector\n",
    "#         context_vect = Vo[context, :];       # context wector\n",
    "#         negative_vects = Vo[neg_context, :]  # sampled negative vectors\n",
    "\n",
    "#         # Cost and gradient calculation starts here\n",
    "#         score_pos = word_vect @ context_vect.T\n",
    "#         score_neg = word_vect @ negative_vects.T\n",
    "\n",
    "#         J -= np.log(sigmoid(score_pos)) + np.sum(np.log(sigmoid(-score_neg)))\n",
    "#         if (i + 1) % conf.log_period == 0:\n",
    "#             print('Update {0}\\tcost: {1:>2.2f}'.format(i + 1, J / conf.log_period))\n",
    "#             final_cost = J / conf.log_period\n",
    "#             J = 0.0\n",
    "\n",
    "#         pos_g = 1.0 - sigmoid(score_pos)\n",
    "#         neg_g = sigmoid(score_neg)\n",
    "\n",
    "#         # Note: use pos_g and neg_g will simplify gradient calculation.\n",
    "#         # raise Exception(\"Unimplemented\")\n",
    "        \n",
    "#         word_grad = -pos_g * context_vect + neg_g @ negative_vects\n",
    "#         context_grad = -pos_g * word_vect\n",
    "#         neg_context_grad = np.outer(neg_g, word_vect)\n",
    "\n",
    "#         Vp[word, :] -= learning_rate * word_grad\n",
    "#         Vo[context, :] -= learning_rate * context_grad\n",
    "#         Vo[neg_context, :] -= learning_rate * neg_context_grad\n",
    "\n",
    "#         if i % conf.decay_period == 0:\n",
    "#             learning_rate = learning_rate * conf.learning_rate_decay\n",
    "\n",
    "#     return Vp, Vo, final_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 10000\tcost: 36.18\n",
      "Update 20000\tcost: 28.55\n",
      "Update 30000\tcost: 23.31\n",
      "Update 40000\tcost: 19.56\n",
      "Update 50000\tcost: 17.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m V_p, V_o, loss \u001b[38;5;241m=\u001b[39m \u001b[43mneg_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mneg_sample\u001b[0;34m(conf, train_set, train_tokens)\u001b[0m\n\u001b[1;32m     27\u001b[0m     final_cost \u001b[38;5;241m=\u001b[39m J \u001b[38;5;241m/\u001b[39m conf\u001b[38;5;241m.\u001b[39mlog_period\n\u001b[1;32m     28\u001b[0m     J \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 30\u001b[0m pos_g \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m neg_g \u001b[38;5;241m=\u001b[39m sigmoid(score_neg)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Note: use pos_g and neg_g will simplify gradient calculation.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# raise Exception(\"Unimplemented\")\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "V_p, V_o, loss = neg_sample(config, train_set, train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookup_word_idx(word, word_dict):\n",
    "    try:\n",
    "        return np.argwhere(np.array(word_dict) == word)[0][0]\n",
    "    except:\n",
    "        raise Exception(\"No such word in dict: {}\".format(word))\n",
    "\n",
    "\n",
    "def similar_words(embeddings, word, word_dict, hits):\n",
    "    word_idx = lookup_word_idx(word, word_dict)\n",
    "    similarity_scores = embeddings @ embeddings[word_idx]\n",
    "    similar_word_idxs = np.argsort(-similarity_scores)\n",
    "    return [word_dict[i] for i in similar_word_idxs[:hits]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training cost: 3.02\n",
      "\n",
      "\n",
      "Words similar to zero: zero, acres, nautical, pmid, hardcover\n",
      "Words similar to computer: computer, computers, hardware, macintosh, amiga\n",
      "Words similar to cars: cars, manufactured, lighter, diesel, collect\n",
      "Words similar to home: home, stadium, cleveland, gardens, houston\n",
      "Words similar to album: album, albums, song, bowie, concert\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nTraining cost: {0:>2.2f}\\n\\n\".format(loss))\n",
    "\n",
    "Vp_norm = V_p / np.linalg.norm(V_p, axis=1).reshape(-1, 1)\n",
    "for w in [\"zero\", \"computer\", \"cars\", \"home\", \"album\"]:\n",
    "    similar = similar_words(Vp_norm, w, train_dict, 5)\n",
    "    print(\"Words similar to {}: {}\".format(w, \", \".join(similar)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
