{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from abc import abstractmethod, ABC\n",
    "from typing import Optional, Literal\n",
    "\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zeros(*dims: int) -> NDArray[np.float32]:\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "\n",
    "def ones(*dims: int) -> NDArray[np.float32]:\n",
    "    return np.ones(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "\n",
    "def rand(*dims: int) -> NDArray[np.float32]:\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "\n",
    "def randn(*dims: int) -> NDArray[np.float32]:\n",
    "    return np.random.randn(*dims).astype(np.float32)\n",
    "\n",
    "\n",
    "def sigmoid(x: NDArray) -> NDArray:\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open(\"text8.dat.gz\", \"rb\") as f:\n",
    "    train_dict, train_set, train_tokens = pickle.load(f)\n",
    "\n",
    "train_set = np.random.permutation(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Config = namedtuple(\n",
    "    \"Config\",\n",
    "    (\n",
    "        \"dict_size\",\n",
    "        \"vect_size\",\n",
    "        \"neg_samples\",\n",
    "        \"updates\",\n",
    "        \"learning_rate\",\n",
    "        \"learning_rate_decay\",\n",
    "        \"decay_period\",\n",
    "        \"log_period\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "config = Config(\n",
    "    dict_size=len(train_dict),\n",
    "    vect_size=100,\n",
    "    neg_samples=10,\n",
    "    updates=5_000_000,\n",
    "    learning_rate=0.1,\n",
    "    learning_rate_decay=0.995,\n",
    "    decay_period=10_000,\n",
    "    log_period=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "In order to train distributed word representations, first:\n",
    " - calculate gradient of the cost function with respect to the `word_vector` and store in `word_grad`.\n",
    " - calculate gradient of the cost function with respect to the `context_vect` and store in `context_grad`.\n",
    " - calculate gradient of the cost function with respect to the sampled `negative_vects` and store in neg_context_grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neg_sample(\n",
    "    config: Config,\n",
    "    train_set: NDArray[np.int64],\n",
    "    train_tokens: NDArray[np.int64],\n",
    ") -> tuple[NDArray, NDArray, float]:\n",
    "    lr = config.learning_rate\n",
    "    loss = 0.0\n",
    "    V_p = randn(config.dict_size, config.vect_size)\n",
    "    V_o = randn(config.dict_size, config.vect_size)\n",
    "\n",
    "    for i in range(config.updates):\n",
    "        w_idx: int = train_set[i % len(train_set), 0]\n",
    "        c_idx: int = train_set[i % len(train_set), 1]\n",
    "        n_idx: NDArray[np.int64] = np.random.randint(0, len(train_tokens), config.neg_samples)\n",
    "        n_idx: NDArray[np.int64] = train_tokens[n_idx]\n",
    "\n",
    "        w = V_p[w_idx, :]  # word vector, shape `(dim,)`\n",
    "        c = V_o[c_idx, :]  # context vector, shape `(dim,)`\n",
    "        n = V_o[n_idx, :]  # sampled noise vectors, shape `(k, dim)`\n",
    "\n",
    "        # Cost and gradient calculation\n",
    "        # -----------------------------\n",
    "        σ_p = sigmoid(+w @ c.T)  # shape `(1,)`\n",
    "        σ_n = sigmoid(-w @ n.T)  # shape `(k,)`\n",
    "        loss -= np.log(σ_p) + np.sum(np.log(σ_n))\n",
    "\n",
    "        if (i + 1) % config.log_period == 0:\n",
    "            print(f\"Update {i+1}\\tLoss: {loss / config.log_period:>2.2f}\")\n",
    "            final_loss = loss / config.log_period\n",
    "            loss = 0.0\n",
    "\n",
    "        grad_w = (σ_p - 1.0) * c + (1.0 - σ_n) @ n\n",
    "        grad_c = (σ_p - 1.0) * w\n",
    "        grad_n = (1.0 - σ_n).reshape(-1, 1) * w\n",
    "\n",
    "        V_p[w_idx, :] -= lr * grad_w\n",
    "        V_o[c_idx, :] -= lr * grad_c\n",
    "        V_o[n_idx, :] -= lr * grad_n\n",
    "\n",
    "        if i % config.decay_period == 0:\n",
    "            lr = lr * config.learning_rate_decay\n",
    "\n",
    "    return V_p, V_o, final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 10000\tLoss: 36.18\n",
      "Update 20000\tLoss: 28.55\n",
      "Update 30000\tLoss: 23.31\n",
      "Update 40000\tLoss: 19.56\n",
      "Update 50000\tLoss: 17.02\n",
      "Update 60000\tLoss: 15.54\n",
      "Update 70000\tLoss: 14.13\n",
      "Update 80000\tLoss: 13.11\n",
      "Update 90000\tLoss: 12.12\n",
      "Update 100000\tLoss: 11.62\n",
      "Update 110000\tLoss: 10.89\n",
      "Update 120000\tLoss: 10.51\n",
      "Update 130000\tLoss: 10.12\n",
      "Update 140000\tLoss: 9.83\n",
      "Update 150000\tLoss: 9.45\n",
      "Update 160000\tLoss: 9.25\n",
      "Update 170000\tLoss: 8.86\n",
      "Update 180000\tLoss: 8.51\n",
      "Update 190000\tLoss: 8.34\n",
      "Update 200000\tLoss: 8.18\n",
      "Update 210000\tLoss: 7.97\n",
      "Update 220000\tLoss: 7.83\n",
      "Update 230000\tLoss: 7.63\n",
      "Update 240000\tLoss: 7.61\n",
      "Update 250000\tLoss: 7.48\n",
      "Update 260000\tLoss: 7.24\n",
      "Update 270000\tLoss: 7.16\n",
      "Update 280000\tLoss: 7.00\n",
      "Update 290000\tLoss: 7.03\n",
      "Update 300000\tLoss: 6.94\n",
      "Update 310000\tLoss: 6.82\n",
      "Update 320000\tLoss: 6.76\n",
      "Update 330000\tLoss: 6.63\n",
      "Update 340000\tLoss: 6.68\n",
      "Update 350000\tLoss: 6.47\n",
      "Update 360000\tLoss: 6.40\n",
      "Update 370000\tLoss: 6.33\n",
      "Update 380000\tLoss: 6.36\n",
      "Update 390000\tLoss: 6.27\n",
      "Update 400000\tLoss: 6.20\n",
      "Update 410000\tLoss: 6.20\n",
      "Update 420000\tLoss: 6.10\n",
      "Update 430000\tLoss: 6.02\n",
      "Update 440000\tLoss: 5.97\n",
      "Update 450000\tLoss: 5.96\n",
      "Update 460000\tLoss: 5.93\n",
      "Update 470000\tLoss: 5.85\n",
      "Update 480000\tLoss: 5.81\n",
      "Update 490000\tLoss: 5.75\n",
      "Update 500000\tLoss: 5.72\n",
      "Update 510000\tLoss: 5.68\n",
      "Update 520000\tLoss: 5.61\n",
      "Update 530000\tLoss: 5.62\n",
      "Update 540000\tLoss: 5.62\n",
      "Update 550000\tLoss: 5.59\n",
      "Update 560000\tLoss: 5.52\n",
      "Update 570000\tLoss: 5.52\n",
      "Update 580000\tLoss: 5.44\n",
      "Update 590000\tLoss: 5.50\n",
      "Update 600000\tLoss: 5.47\n",
      "Update 610000\tLoss: 5.38\n",
      "Update 620000\tLoss: 5.28\n",
      "Update 630000\tLoss: 5.34\n",
      "Update 640000\tLoss: 5.34\n",
      "Update 650000\tLoss: 5.29\n",
      "Update 660000\tLoss: 5.27\n",
      "Update 670000\tLoss: 5.27\n",
      "Update 680000\tLoss: 5.20\n",
      "Update 690000\tLoss: 5.09\n",
      "Update 700000\tLoss: 5.18\n",
      "Update 710000\tLoss: 5.10\n",
      "Update 720000\tLoss: 5.07\n",
      "Update 730000\tLoss: 5.08\n",
      "Update 740000\tLoss: 5.02\n",
      "Update 750000\tLoss: 5.04\n",
      "Update 760000\tLoss: 5.07\n",
      "Update 770000\tLoss: 5.02\n",
      "Update 780000\tLoss: 4.98\n",
      "Update 790000\tLoss: 4.99\n",
      "Update 800000\tLoss: 4.91\n",
      "Update 810000\tLoss: 4.91\n",
      "Update 820000\tLoss: 4.89\n",
      "Update 830000\tLoss: 4.91\n",
      "Update 840000\tLoss: 4.86\n",
      "Update 850000\tLoss: 4.83\n",
      "Update 860000\tLoss: 4.81\n",
      "Update 870000\tLoss: 4.83\n",
      "Update 880000\tLoss: 4.83\n",
      "Update 890000\tLoss: 4.75\n",
      "Update 900000\tLoss: 4.78\n",
      "Update 910000\tLoss: 4.71\n",
      "Update 920000\tLoss: 4.70\n",
      "Update 930000\tLoss: 4.73\n",
      "Update 940000\tLoss: 4.68\n",
      "Update 950000\tLoss: 4.67\n",
      "Update 960000\tLoss: 4.66\n",
      "Update 970000\tLoss: 4.66\n",
      "Update 980000\tLoss: 4.62\n",
      "Update 990000\tLoss: 4.61\n",
      "Update 1000000\tLoss: 4.63\n",
      "Update 1010000\tLoss: 4.65\n",
      "Update 1020000\tLoss: 4.59\n",
      "Update 1030000\tLoss: 4.60\n",
      "Update 1040000\tLoss: 4.55\n",
      "Update 1050000\tLoss: 4.55\n",
      "Update 1060000\tLoss: 4.56\n",
      "Update 1070000\tLoss: 4.61\n",
      "Update 1080000\tLoss: 4.51\n",
      "Update 1090000\tLoss: 4.53\n",
      "Update 1100000\tLoss: 4.46\n",
      "Update 1110000\tLoss: 4.46\n",
      "Update 1120000\tLoss: 4.43\n",
      "Update 1130000\tLoss: 4.50\n",
      "Update 1140000\tLoss: 4.43\n",
      "Update 1150000\tLoss: 4.53\n",
      "Update 1160000\tLoss: 4.41\n",
      "Update 1170000\tLoss: 4.44\n",
      "Update 1180000\tLoss: 4.41\n",
      "Update 1190000\tLoss: 4.41\n",
      "Update 1200000\tLoss: 4.37\n",
      "Update 1210000\tLoss: 4.40\n",
      "Update 1220000\tLoss: 4.36\n",
      "Update 1230000\tLoss: 4.36\n",
      "Update 1240000\tLoss: 4.36\n",
      "Update 1250000\tLoss: 4.35\n",
      "Update 1260000\tLoss: 4.33\n",
      "Update 1270000\tLoss: 4.35\n",
      "Update 1280000\tLoss: 4.33\n",
      "Update 1290000\tLoss: 4.32\n",
      "Update 1300000\tLoss: 4.31\n",
      "Update 1310000\tLoss: 4.29\n",
      "Update 1320000\tLoss: 4.28\n",
      "Update 1330000\tLoss: 4.30\n",
      "Update 1340000\tLoss: 4.23\n",
      "Update 1350000\tLoss: 4.23\n",
      "Update 1360000\tLoss: 4.26\n",
      "Update 1370000\tLoss: 4.23\n",
      "Update 1380000\tLoss: 4.22\n",
      "Update 1390000\tLoss: 4.21\n",
      "Update 1400000\tLoss: 4.23\n",
      "Update 1410000\tLoss: 4.28\n",
      "Update 1420000\tLoss: 4.19\n",
      "Update 1430000\tLoss: 4.22\n",
      "Update 1440000\tLoss: 4.18\n",
      "Update 1450000\tLoss: 4.15\n",
      "Update 1460000\tLoss: 4.21\n",
      "Update 1470000\tLoss: 4.17\n",
      "Update 1480000\tLoss: 4.17\n",
      "Update 1490000\tLoss: 4.13\n",
      "Update 1500000\tLoss: 4.13\n",
      "Update 1510000\tLoss: 4.17\n",
      "Update 1520000\tLoss: 4.09\n",
      "Update 1530000\tLoss: 4.17\n",
      "Update 1540000\tLoss: 4.12\n",
      "Update 1550000\tLoss: 4.16\n",
      "Update 1560000\tLoss: 4.12\n",
      "Update 1570000\tLoss: 4.12\n",
      "Update 1580000\tLoss: 4.11\n",
      "Update 1590000\tLoss: 4.15\n",
      "Update 1600000\tLoss: 4.06\n",
      "Update 1610000\tLoss: 4.07\n",
      "Update 1620000\tLoss: 4.08\n",
      "Update 1630000\tLoss: 4.06\n",
      "Update 1640000\tLoss: 4.05\n",
      "Update 1650000\tLoss: 4.09\n",
      "Update 1660000\tLoss: 4.03\n",
      "Update 1670000\tLoss: 4.03\n",
      "Update 1680000\tLoss: 4.03\n",
      "Update 1690000\tLoss: 4.06\n",
      "Update 1700000\tLoss: 4.04\n",
      "Update 1710000\tLoss: 4.01\n",
      "Update 1720000\tLoss: 4.01\n",
      "Update 1730000\tLoss: 4.04\n",
      "Update 1740000\tLoss: 4.03\n",
      "Update 1750000\tLoss: 4.01\n",
      "Update 1760000\tLoss: 4.01\n",
      "Update 1770000\tLoss: 4.02\n",
      "Update 1780000\tLoss: 4.01\n",
      "Update 1790000\tLoss: 4.02\n",
      "Update 1800000\tLoss: 3.99\n",
      "Update 1810000\tLoss: 3.97\n",
      "Update 1820000\tLoss: 3.96\n",
      "Update 1830000\tLoss: 3.97\n",
      "Update 1840000\tLoss: 3.93\n",
      "Update 1850000\tLoss: 3.96\n",
      "Update 1860000\tLoss: 3.96\n",
      "Update 1870000\tLoss: 3.97\n",
      "Update 1880000\tLoss: 3.90\n",
      "Update 1890000\tLoss: 3.94\n",
      "Update 1900000\tLoss: 3.94\n",
      "Update 1910000\tLoss: 3.92\n",
      "Update 1920000\tLoss: 3.89\n",
      "Update 1930000\tLoss: 3.89\n",
      "Update 1940000\tLoss: 3.93\n",
      "Update 1950000\tLoss: 3.93\n",
      "Update 1960000\tLoss: 3.96\n",
      "Update 1970000\tLoss: 3.92\n",
      "Update 1980000\tLoss: 3.91\n",
      "Update 1990000\tLoss: 3.92\n",
      "Update 2000000\tLoss: 3.91\n",
      "Update 2010000\tLoss: 3.88\n",
      "Update 2020000\tLoss: 3.89\n",
      "Update 2030000\tLoss: 3.88\n",
      "Update 2040000\tLoss: 3.90\n",
      "Update 2050000\tLoss: 3.84\n",
      "Update 2060000\tLoss: 3.90\n",
      "Update 2070000\tLoss: 3.89\n",
      "Update 2080000\tLoss: 3.83\n",
      "Update 2090000\tLoss: 3.87\n",
      "Update 2100000\tLoss: 3.90\n",
      "Update 2110000\tLoss: 3.88\n",
      "Update 2120000\tLoss: 3.88\n",
      "Update 2130000\tLoss: 3.90\n",
      "Update 2140000\tLoss: 3.84\n",
      "Update 2150000\tLoss: 3.85\n",
      "Update 2160000\tLoss: 3.88\n",
      "Update 2170000\tLoss: 3.83\n",
      "Update 2180000\tLoss: 3.87\n",
      "Update 2190000\tLoss: 3.88\n",
      "Update 2200000\tLoss: 3.82\n",
      "Update 2210000\tLoss: 3.82\n",
      "Update 2220000\tLoss: 3.84\n",
      "Update 2230000\tLoss: 3.79\n",
      "Update 2240000\tLoss: 3.85\n",
      "Update 2250000\tLoss: 3.83\n",
      "Update 2260000\tLoss: 3.81\n",
      "Update 2270000\tLoss: 3.80\n",
      "Update 2280000\tLoss: 3.78\n",
      "Update 2290000\tLoss: 3.83\n",
      "Update 2300000\tLoss: 3.80\n",
      "Update 2310000\tLoss: 3.80\n",
      "Update 2320000\tLoss: 3.78\n",
      "Update 2330000\tLoss: 3.78\n",
      "Update 2340000\tLoss: 3.76\n",
      "Update 2350000\tLoss: 3.77\n",
      "Update 2360000\tLoss: 3.80\n",
      "Update 2370000\tLoss: 3.80\n",
      "Update 2380000\tLoss: 3.73\n",
      "Update 2390000\tLoss: 3.76\n",
      "Update 2400000\tLoss: 3.76\n",
      "Update 2410000\tLoss: 3.74\n",
      "Update 2420000\tLoss: 3.75\n",
      "Update 2430000\tLoss: 3.77\n",
      "Update 2440000\tLoss: 3.76\n",
      "Update 2450000\tLoss: 3.76\n",
      "Update 2460000\tLoss: 3.74\n",
      "Update 2470000\tLoss: 3.72\n",
      "Update 2480000\tLoss: 3.74\n",
      "Update 2490000\tLoss: 3.77\n",
      "Update 2500000\tLoss: 3.73\n",
      "Update 2510000\tLoss: 3.75\n",
      "Update 2520000\tLoss: 3.74\n",
      "Update 2530000\tLoss: 3.72\n",
      "Update 2540000\tLoss: 3.75\n",
      "Update 2550000\tLoss: 3.71\n",
      "Update 2560000\tLoss: 3.72\n",
      "Update 2570000\tLoss: 3.73\n",
      "Update 2580000\tLoss: 3.72\n",
      "Update 2590000\tLoss: 3.72\n",
      "Update 2600000\tLoss: 3.69\n",
      "Update 2610000\tLoss: 3.72\n",
      "Update 2620000\tLoss: 3.69\n",
      "Update 2630000\tLoss: 3.73\n",
      "Update 2640000\tLoss: 3.70\n",
      "Update 2650000\tLoss: 3.71\n",
      "Update 2660000\tLoss: 3.72\n",
      "Update 2670000\tLoss: 3.73\n",
      "Update 2680000\tLoss: 3.72\n",
      "Update 2690000\tLoss: 3.67\n",
      "Update 2700000\tLoss: 3.71\n",
      "Update 2710000\tLoss: 3.71\n",
      "Update 2720000\tLoss: 3.69\n",
      "Update 2730000\tLoss: 3.70\n",
      "Update 2740000\tLoss: 3.63\n",
      "Update 2750000\tLoss: 3.68\n",
      "Update 2760000\tLoss: 3.65\n",
      "Update 2770000\tLoss: 3.71\n",
      "Update 2780000\tLoss: 3.68\n",
      "Update 2790000\tLoss: 3.67\n",
      "Update 2800000\tLoss: 3.68\n",
      "Update 2810000\tLoss: 3.66\n",
      "Update 2820000\tLoss: 3.68\n",
      "Update 2830000\tLoss: 3.66\n",
      "Update 2840000\tLoss: 3.72\n",
      "Update 2850000\tLoss: 3.68\n",
      "Update 2860000\tLoss: 3.69\n",
      "Update 2870000\tLoss: 3.65\n",
      "Update 2880000\tLoss: 3.68\n",
      "Update 2890000\tLoss: 3.67\n",
      "Update 2900000\tLoss: 3.63\n",
      "Update 2910000\tLoss: 3.65\n",
      "Update 2920000\tLoss: 3.62\n",
      "Update 2930000\tLoss: 3.66\n",
      "Update 2940000\tLoss: 3.66\n",
      "Update 2950000\tLoss: 3.66\n",
      "Update 2960000\tLoss: 3.63\n",
      "Update 2970000\tLoss: 3.63\n",
      "Update 2980000\tLoss: 3.64\n",
      "Update 2990000\tLoss: 3.62\n",
      "Update 3000000\tLoss: 3.64\n",
      "Update 3010000\tLoss: 3.61\n",
      "Update 3020000\tLoss: 3.65\n",
      "Update 3030000\tLoss: 3.62\n",
      "Update 3040000\tLoss: 3.59\n",
      "Update 3050000\tLoss: 3.63\n",
      "Update 3060000\tLoss: 3.62\n",
      "Update 3070000\tLoss: 3.63\n",
      "Update 3080000\tLoss: 3.61\n",
      "Update 3090000\tLoss: 3.62\n",
      "Update 3100000\tLoss: 3.66\n",
      "Update 3110000\tLoss: 3.64\n",
      "Update 3120000\tLoss: 3.63\n",
      "Update 3130000\tLoss: 3.61\n",
      "Update 3140000\tLoss: 3.62\n",
      "Update 3150000\tLoss: 3.59\n",
      "Update 3160000\tLoss: 3.62\n",
      "Update 3170000\tLoss: 3.62\n",
      "Update 3180000\tLoss: 3.64\n",
      "Update 3190000\tLoss: 3.60\n",
      "Update 3200000\tLoss: 3.59\n",
      "Update 3210000\tLoss: 3.60\n",
      "Update 3220000\tLoss: 3.61\n",
      "Update 3230000\tLoss: 3.61\n",
      "Update 3240000\tLoss: 3.62\n",
      "Update 3250000\tLoss: 3.58\n",
      "Update 3260000\tLoss: 3.59\n",
      "Update 3270000\tLoss: 3.60\n",
      "Update 3280000\tLoss: 3.59\n",
      "Update 3290000\tLoss: 3.58\n",
      "Update 3300000\tLoss: 3.61\n",
      "Update 3310000\tLoss: 3.61\n",
      "Update 3320000\tLoss: 3.62\n",
      "Update 3330000\tLoss: 3.61\n",
      "Update 3340000\tLoss: 3.59\n",
      "Update 3350000\tLoss: 3.60\n",
      "Update 3360000\tLoss: 3.57\n",
      "Update 3370000\tLoss: 3.56\n",
      "Update 3380000\tLoss: 3.58\n",
      "Update 3390000\tLoss: 3.59\n",
      "Update 3400000\tLoss: 3.56\n",
      "Update 3410000\tLoss: 3.58\n",
      "Update 3420000\tLoss: 3.58\n",
      "Update 3430000\tLoss: 3.63\n",
      "Update 3440000\tLoss: 3.59\n",
      "Update 3450000\tLoss: 3.57\n",
      "Update 3460000\tLoss: 3.56\n",
      "Update 3470000\tLoss: 3.58\n",
      "Update 3480000\tLoss: 3.56\n",
      "Update 3490000\tLoss: 3.57\n",
      "Update 3500000\tLoss: 3.57\n",
      "Update 3510000\tLoss: 3.57\n",
      "Update 3520000\tLoss: 3.55\n",
      "Update 3530000\tLoss: 3.60\n",
      "Update 3540000\tLoss: 3.59\n",
      "Update 3550000\tLoss: 3.57\n",
      "Update 3560000\tLoss: 3.58\n",
      "Update 3570000\tLoss: 3.56\n",
      "Update 3580000\tLoss: 3.55\n",
      "Update 3590000\tLoss: 3.55\n",
      "Update 3600000\tLoss: 3.56\n",
      "Update 3610000\tLoss: 3.57\n",
      "Update 3620000\tLoss: 3.55\n",
      "Update 3630000\tLoss: 3.55\n",
      "Update 3640000\tLoss: 3.56\n",
      "Update 3650000\tLoss: 3.55\n",
      "Update 3660000\tLoss: 3.53\n",
      "Update 3670000\tLoss: 3.57\n",
      "Update 3680000\tLoss: 3.54\n",
      "Update 3690000\tLoss: 3.52\n",
      "Update 3700000\tLoss: 3.52\n",
      "Update 3710000\tLoss: 3.55\n",
      "Update 3720000\tLoss: 3.57\n",
      "Update 3730000\tLoss: 3.55\n",
      "Update 3740000\tLoss: 3.55\n",
      "Update 3750000\tLoss: 3.53\n",
      "Update 3760000\tLoss: 3.55\n",
      "Update 3770000\tLoss: 3.54\n",
      "Update 3780000\tLoss: 3.52\n",
      "Update 3790000\tLoss: 3.54\n",
      "Update 3800000\tLoss: 3.52\n",
      "Update 3810000\tLoss: 3.55\n",
      "Update 3820000\tLoss: 3.55\n",
      "Update 3830000\tLoss: 3.53\n",
      "Update 3840000\tLoss: 3.51\n",
      "Update 3850000\tLoss: 3.52\n",
      "Update 3860000\tLoss: 3.54\n",
      "Update 3870000\tLoss: 3.53\n",
      "Update 3880000\tLoss: 3.54\n",
      "Update 3890000\tLoss: 3.56\n",
      "Update 3900000\tLoss: 3.55\n",
      "Update 3910000\tLoss: 3.51\n",
      "Update 3920000\tLoss: 3.51\n",
      "Update 3930000\tLoss: 3.55\n",
      "Update 3940000\tLoss: 3.53\n",
      "Update 3950000\tLoss: 3.52\n",
      "Update 3960000\tLoss: 3.53\n",
      "Update 3970000\tLoss: 3.51\n",
      "Update 3980000\tLoss: 3.51\n",
      "Update 3990000\tLoss: 3.54\n",
      "Update 4000000\tLoss: 3.51\n",
      "Update 4010000\tLoss: 3.52\n",
      "Update 4020000\tLoss: 3.52\n",
      "Update 4030000\tLoss: 3.51\n",
      "Update 4040000\tLoss: 3.53\n",
      "Update 4050000\tLoss: 3.51\n",
      "Update 4060000\tLoss: 3.50\n",
      "Update 4070000\tLoss: 3.50\n",
      "Update 4080000\tLoss: 3.50\n",
      "Update 4090000\tLoss: 3.51\n",
      "Update 4100000\tLoss: 3.52\n",
      "Update 4110000\tLoss: 3.50\n",
      "Update 4120000\tLoss: 3.51\n",
      "Update 4130000\tLoss: 3.52\n",
      "Update 4140000\tLoss: 3.51\n",
      "Update 4150000\tLoss: 3.48\n",
      "Update 4160000\tLoss: 3.48\n",
      "Update 4170000\tLoss: 3.52\n",
      "Update 4180000\tLoss: 3.49\n",
      "Update 4190000\tLoss: 3.52\n",
      "Update 4200000\tLoss: 3.55\n",
      "Update 4210000\tLoss: 3.52\n",
      "Update 4220000\tLoss: 3.54\n",
      "Update 4230000\tLoss: 3.51\n",
      "Update 4240000\tLoss: 3.50\n",
      "Update 4250000\tLoss: 3.49\n",
      "Update 4260000\tLoss: 3.48\n",
      "Update 4270000\tLoss: 3.52\n",
      "Update 4280000\tLoss: 3.49\n",
      "Update 4290000\tLoss: 3.50\n",
      "Update 4300000\tLoss: 3.47\n",
      "Update 4310000\tLoss: 3.49\n",
      "Update 4320000\tLoss: 3.48\n",
      "Update 4330000\tLoss: 3.45\n",
      "Update 4340000\tLoss: 3.51\n",
      "Update 4350000\tLoss: 3.46\n",
      "Update 4360000\tLoss: 3.50\n",
      "Update 4370000\tLoss: 3.49\n",
      "Update 4380000\tLoss: 3.50\n",
      "Update 4390000\tLoss: 3.49\n",
      "Update 4400000\tLoss: 3.50\n",
      "Update 4410000\tLoss: 3.46\n",
      "Update 4420000\tLoss: 3.47\n",
      "Update 4430000\tLoss: 3.50\n",
      "Update 4440000\tLoss: 3.46\n",
      "Update 4450000\tLoss: 3.50\n",
      "Update 4460000\tLoss: 3.48\n",
      "Update 4470000\tLoss: 3.48\n",
      "Update 4480000\tLoss: 3.48\n",
      "Update 4490000\tLoss: 3.51\n",
      "Update 4500000\tLoss: 3.49\n",
      "Update 4510000\tLoss: 3.48\n",
      "Update 4520000\tLoss: 3.46\n",
      "Update 4530000\tLoss: 3.47\n",
      "Update 4540000\tLoss: 3.48\n",
      "Update 4550000\tLoss: 3.46\n",
      "Update 4560000\tLoss: 3.48\n",
      "Update 4570000\tLoss: 3.47\n",
      "Update 4580000\tLoss: 3.47\n",
      "Update 4590000\tLoss: 3.45\n",
      "Update 4600000\tLoss: 3.48\n",
      "Update 4610000\tLoss: 3.49\n",
      "Update 4620000\tLoss: 3.47\n",
      "Update 4630000\tLoss: 3.50\n",
      "Update 4640000\tLoss: 3.45\n",
      "Update 4650000\tLoss: 3.47\n",
      "Update 4660000\tLoss: 3.49\n",
      "Update 4670000\tLoss: 3.46\n",
      "Update 4680000\tLoss: 3.47\n",
      "Update 4690000\tLoss: 3.47\n",
      "Update 4700000\tLoss: 3.46\n",
      "Update 4710000\tLoss: 3.49\n",
      "Update 4720000\tLoss: 3.48\n",
      "Update 4730000\tLoss: 3.47\n",
      "Update 4740000\tLoss: 3.48\n",
      "Update 4750000\tLoss: 3.44\n",
      "Update 4760000\tLoss: 3.48\n",
      "Update 4770000\tLoss: 3.44\n",
      "Update 4780000\tLoss: 3.47\n",
      "Update 4790000\tLoss: 3.48\n",
      "Update 4800000\tLoss: 3.46\n",
      "Update 4810000\tLoss: 3.47\n",
      "Update 4820000\tLoss: 3.46\n",
      "Update 4830000\tLoss: 3.45\n",
      "Update 4840000\tLoss: 3.47\n",
      "Update 4850000\tLoss: 3.46\n",
      "Update 4860000\tLoss: 3.46\n",
      "Update 4870000\tLoss: 3.45\n",
      "Update 4880000\tLoss: 3.43\n",
      "Update 4890000\tLoss: 3.44\n",
      "Update 4900000\tLoss: 3.46\n",
      "Update 4910000\tLoss: 3.45\n",
      "Update 4920000\tLoss: 3.44\n",
      "Update 4930000\tLoss: 3.45\n",
      "Update 4940000\tLoss: 3.45\n",
      "Update 4950000\tLoss: 3.45\n",
      "Update 4960000\tLoss: 3.47\n",
      "Update 4970000\tLoss: 3.43\n",
      "Update 4980000\tLoss: 3.44\n",
      "Update 4990000\tLoss: 3.47\n",
      "Update 5000000\tLoss: 3.43\n"
     ]
    }
   ],
   "source": [
    "V_p, V_o, loss = neg_sample(config, train_set, train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookup_word_idx(word, word_dict):\n",
    "    try:\n",
    "        return np.argwhere(np.array(word_dict) == word)[0][0]\n",
    "    except:\n",
    "        raise Exception(\"No such word in dict: {}\".format(word))\n",
    "\n",
    "\n",
    "def similar_words(embeddings, word, word_dict, hits):\n",
    "    word_idx = lookup_word_idx(word, word_dict)\n",
    "    similarity_scores = embeddings @ embeddings[word_idx]\n",
    "    similar_word_idxs = np.argsort(-similarity_scores)\n",
    "    return [word_dict[i] for i in similar_word_idxs[:hits]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training cost: 3.43\n",
      "\n",
      "\n",
      "Words similar to zero: zero, four, three, five, seven\n",
      "Words similar to computer: computer, software, applications, video, systems\n",
      "Words similar to cars: cars, vehicles, built, players, electric\n",
      "Words similar to home: home, city, park, building, players\n",
      "Words similar to album: album, released, band, appeared, song\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nTraining cost: {0:>2.2f}\\n\\n\".format(loss))\n",
    "\n",
    "Vp_norm = V_p / np.linalg.norm(V_p, axis=1).reshape(-1, 1)\n",
    "for w in [\"zero\", \"computer\", \"cars\", \"home\", \"album\"]:\n",
    "    similar = similar_words(Vp_norm, w, train_dict, 5)\n",
    "    print(\"Words similar to {}: {}\".format(w, \", \".join(similar)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
